---
title: "Time DAG PostgreSQL"
output: html_document
---



Query sequences joined to themselves blow up the query complexity exponentially, as each path is re-build on each attempted re-use.  Here we will work a deliberately nasty example.  We are going to push hard on the infrastructure to see how strong it is.

First we set up a `PostgreSQL` example in `R`.

```{r}
library("rquery")
packageVersion("rquery")
library("wrapr")

raw_connection <- DBI::dbConnect(RPostgreSQL::PostgreSQL(),
                                 host = 'localhost',
                                 port = 5432,
                                 user = 'johnmount',
                                 password = '')

dbopts <- rq_connection_tests(raw_connection)
db_rquery <- rquery_db_info(connection = raw_connection,
                     is_dbi = TRUE,
                     connection_options = dbopts)

d <- data.frame(x = paste0("v_", 1:100000),
                stringsAsFactors = FALSE)

d0 <- rq_copy_to(db_rquery, "d", d,
                 temporary = TRUE, 
                 overwrite = TRUE)

d1 <- natural_join(d0, d0, by = "x", jointype = "LEFT")
d2 <- natural_join(d1, d1, by = "x", jointype = "LEFT")
d3 <- natural_join(d2, d2, by = "x", jointype = "LEFT")

cat(format(d3))
```

Notice the depth 3 expression exploded into tree with 7 joins.  This may be easier to see in the following diagram.

```{r diagram1, fig.width=8, fig.height=8}
d3 %.>%
  op_diagram(., merge_tables = TRUE) %.>% 
  DiagrammeR::grViz(.) %.>%
  DiagrammeRsvg::export_svg(.) %.>%
  write(., file="time_dag_diagram1p.svg")
```

![](time_dag_diagram1p.svg)

Notice the directed acyclic graph of three joins exploded into diagram of seven joins (due to the re-computation of intermediate results).  This is the consequence of re-using results in SQL (without the the use of intermediate tables or common tabel expressions).

This is not unique to [`rquery`](https://CRAN.R-project.org/package=rquery), [`dplyr`](https://CRAN.R-project.org/package=dplyr) has the same issue.

```{r}
library("dplyr")
packageVersion("dplyr")
packageVersion("dbplyr")

d0_dplyr <- tbl(raw_connection, "d")

d1_dplyr <- left_join(d0_dplyr, d0_dplyr, by = "x")
d2_dplyr <- left_join(d1_dplyr, d1_dplyr, by = "x")
d3_dplyr <- left_join(d2_dplyr, d2_dplyr, by = "x")
dbplyr::remote_query(d3_dplyr)
```

The expanded query again contains seven joins.

Largely it is a lack of a convenient way to name and cache the intermediate results in basic `SQL` without landing a table or view and starting a new query.  Without value re-use, re-writing a directed-acyclic graph (the specified input) into a tree (the basis of `SQL`) can cause a query explosion.

Now a query planner may be able to eliminate the redundant steps- but that same planner is also facing a query that has size exponential in the query depth.

`dplyr` can easily overcome this limitation with it's `compute()` node.

```{r}
d1_dplyr <- compute(left_join(d0_dplyr, d0_dplyr, by = "x"))
d2_dplyr <- compute(left_join(d1_dplyr, d1_dplyr, by = "x"))
d3_dplyr <- compute(left_join(d2_dplyr, d2_dplyr, by = "x"))
dbplyr::remote_query(d3_dplyr)
```

`rquery` can also fix the issue by landing intermediate results, though the table lifetime tracking is intentionally more explicit through either a [`materialize()`](https://winvector.github.io/rquery/reference/materialize.html) or [`relop_list`](https://winvector.github.io/rquery/reference/relop_list-class.html) step. With a more advanced "collector" notation we can both build the efficient query plan, but also the diagram certifying the lack of redundant stages.

```{r}
tmps <- wrapr::mk_tmp_name_source()
collector <- make_relop_list(tmps)

d1_tab <- natural_join(d0, d0, by = "x", jointype = "LEFT") %.>%
  collector
d2_tab <- natural_join(d1_tab, d1_tab, by = "x", jointype = "LEFT") %.>%
  collector
d3_tab <- natural_join(d2_tab, d2_tab, by = "x", jointype = "LEFT") %.>%
  collector

stages <- get_relop_list_stages(collector)
```

```{r diagram2, fig.width=8, fig.height=8}
stages %.>%
  op_diagram(., merge_tables = TRUE) %.>% 
  DiagrammeR::grViz(.) %.>%
  DiagrammeRsvg::export_svg(.) %.>%
  write(., file="time_dag_diagram2p.svg")
```

![](time_dag_diagram2p.svg)

We can also time the various methods.  For each function we are re-creating the table to try and defeat the result cache and ensure the calculation actually runs (i.e. to get timings similar to what a user would see on first application). For intermediate results the query planner may or may not eliminate redundant calculations at will.

```{r fns}
depth <- 6

f_dplyr <- function() {
  d_dplyr <- dplyr::copy_to(raw_connection, d, 
                            overwrite = TRUE,
                            temporary = TRUE)
  for(i in 1:depth) {
    d_dplyr <- left_join(d_dplyr, d_dplyr, by = "x")
  }
  compute(d_dplyr)
}

f_dplyr_compute <- function() {
  d_dplyr <- dplyr::copy_to(raw_connection, d, 
                            overwrite = TRUE,
                            temporary = TRUE)
  for(i in 1:depth) {
    d_dplyr <- compute(left_join(d_dplyr, d_dplyr, by = "x"))
  }
  d_dplyr
}

f_rquery <- function() {
  d_rquery <- rq_copy_to(db_rquery, "d", d, 
                         temporary = TRUE,
                         overwrite = TRUE)
  for(i in 1:depth) {
    d_rquery <- natural_join(d_rquery, d_rquery, by = "x", jointype = "LEFT")
  }
  materialize(db_rquery, d3)
}

f_rquery_materialize <- function() {
  d_rquery <- rq_copy_to(db_rquery, "d", d, 
                         temporary = TRUE,
                         overwrite = TRUE)
  for(i in 1:depth) {
    d_rquery <- materialize(db_rquery,
                            natural_join(d_rquery, d_rquery, by = "x", jointype = "LEFT"))
  }
  d_rquery
}
```



```{r timings}
timings <- microbenchmark::microbenchmark(
  dplyr = f_dplyr(),
  dplyr_compute = f_dplyr_compute(),
  rquery = f_rquery(),
  rquery_materialize = f_rquery_materialize(),
  times = 5L)

timings
```

I would say concentrate on the mean and median times (as they are similar).  The caching slows things down.  This is likely due to the `PostgreSQL` query optimizer/planner identifying and eliminating common sub-queries.  Though to do so it must survive the query analysis phase.

The above may seem extreme, but in our experience we have seen teams working with database systems through automatic query generators spend a *lot* of time running into and debugging very opaque query growth problems.  The issues include that long sequences of operations get translated into very deep nested queries *and* any re-use of intermediate values translates into unexpected (and not-signaled) query explosion.  Some things that are cheap in immediate/imperative systems are in fact hard in delayed evaluation systems (so common intuition fails). Our hope is that with diagramming tools such as `rquery::op_diagram()` users can anticipate the issues tune their calculation plans using methods such as `rquery::materialize()` and `rquery::relop_list()`.


For a non-trivial example of computation management and value re-use please see [here](https://github.com/WinVector/rquery/blob/master/db_examples/RSQLite.md).
Some more discussion of the query explosion effect is available [here](https://github.com/WinVector/rquery/blob/master/extras/query_growth/query_growth.md).

```{r}
library("rqdatatable")

summary <- data.frame(timings) %.>% 
  project(., 
          time_seconds := median(time)/1e+9,
          groupby = "expr") 
print(summary)
```



```{r}
# clean up tmps
intermediates <- tmps(dumpList = TRUE)
for(ti in intermediates) {
  rquery::rq_remove_table(raw_connection, ti)
}

DBI::dbDisconnect(raw_connection)
rm(list = c("raw_connection", "db_rquery"))
```
