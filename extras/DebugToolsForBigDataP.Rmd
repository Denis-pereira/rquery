---
title: "Debug Tools for Big Data (PostgreSQL)"
output: github_document
date: "2018-05-11"
---

<!-- file.md is generated from file.Rmd. Please edit that file -->


```{r confconnect}
base::date()

library("dplyr")
library("rquery")

my_db <- DBI::dbConnect(RPostgreSQL::PostgreSQL(),
                        host = 'localhost',
                        port = 5432,
                        user = 'johnmount',
                        password = '')

# configure rquery options
dbopts <- dbi_connection_tests(
  my_db,
  # seeing a bug here in RPostgreSQL, possibly on CREATE queries
  overrides = c(use_DBI_dbExistsTable = FALSE, use_DBI_dbRemoveTable = FALSE) 
)
print(dbopts)
options(dbopts)

base::date()
```

```{r startexample}
base::date()

# build up example data
nSubj <- 1000000
nIrrelCol <- 500

d_local <- data.frame(subjectID = sort(rep(seq_len(nSubj),2)),
                 surveyCategory = c(
                   'withdrawal behavior',
                   'positive re-framing'),
                 stringsAsFactors = FALSE)
d_local$assessmentTotal <- sample.int(10, nrow(d_local), replace = TRUE)
d_small <- rquery::dbi_copy_to(my_db, 'd_small',
                 d_local,
                 overwrite = TRUE, 
                 temporary = TRUE)
rm(list = "d_local")
# cdata::qlook(my_db, d_small$table_name)

base::date()
```

```{r growexample}
base::date()

# add in irrelevant columns
# simulates performing a calculation against a larger data mart
assignments <- 
  vapply(seq_len(nIrrelCol), 
         function(i) {
           paste("irrelevantCol", sprintf("%07g", i), sep = "_")
         }, character(1)) := rep("1.5", nIrrelCol)
d_large <- d_small %.>%
  extend_se(., assignments) %.>%
  materialize(my_db, ., 
              overwrite = TRUE,
              temporary = TRUE)
rm(list = "d_small")
# cdata::qlook(my_db, d_large$table_name)

# build dplyr reference
d_large_tbl <- tbl(my_db, d_large$table_name)

# rquery view of table
rquery::dbi_nrow(my_db, d_large$table_name)
length(column_names(d_large))

base::date()
```

Define and demonstrate pipelines:

```{r rqueryexp}
base::date()

scale <- 0.237

rquery_pipeline <- d_large %.>%
  extend_nse(.,
             probability :=
               exp(assessmentTotal * scale))  %.>% 
  normalize_cols(.,
                 "probability",
                 partitionby = 'subjectID') %.>%
  pick_top_k(.,
             partitionby = 'subjectID',
             rev_orderby = c('probability', 'surveyCategory')) %.>%
  rename_columns(., 'diagnosis' := 'surveyCategory') %.>%
  select_columns(., qc(subjectID, diagnosis, probability)) %.>%
  orderby(., 'subjectID') 

# special debug-mode limits all sources to 1 row.
# not correct for windowed calculations or joins- 
# but lets us at least see something execute quickly.
system.time(nrow(as.data.frame(execute(my_db, rquery_pipeline, source_limit = 1L))))

# full run
system.time(nrow(as.data.frame(execute(my_db, rquery_pipeline))))

base::date()
```

```{r dplyrexp}
base::date()

scale <- 0.237

dplyr_pipeline <- d_large_tbl %>%
  group_by(subjectID) %>%
  mutate(probability =
           exp(assessmentTotal * scale)/
           sum(exp(assessmentTotal * scale), na.rm = TRUE)) %>%
  arrange(probability, surveyCategory) %>%
  filter(row_number() == n()) %>%
  ungroup() %>%
  rename(diagnosis = surveyCategory) %>%
  select(subjectID, diagnosis, probability) %>%
  arrange(subjectID)


# full run
system.time(nrow(as.data.frame(dplyr_pipeline)))

base::date()
```

`rquery` `materialize_node()` (`rquery`'s caching node)
works with `rquery`'s column narrowing calculations.

```{r rqueryexp_cache}
base::date()

scale <- 0.237

rquery_pipeline_cached <- d_large %.>%
  extend_nse(.,
             probability :=
               exp(assessmentTotal * scale))  %.>% 
  normalize_cols(.,
                 "probability",
                 partitionby = 'subjectID') %.>%
  pick_top_k(.,
             partitionby = 'subjectID',
             rev_orderby = c('probability', 'surveyCategory')) %.>%
  materialize_node(., "tmp_res") %.>%  # <- insert caching node into pipeline prior to narrowing
  rename_columns(., 'diagnosis' := 'surveyCategory') %.>%
  select_columns(., qc(subjectID, diagnosis, probability)) %.>%
  orderby(., 'subjectID') 

sql_list <- to_sql(rquery_pipeline_cached, my_db)
for(i in seq_len(length(sql_list))) {
  print(paste("step", i))
  cat(format(sql_list[[i]]))
  cat("\n")
}

# special debug-mode limits all sources to 1 row.
# not correct for windowed calculations or joins- 
# but lets us at least see something execute quickly.
system.time(nrow(as.data.frame(execute(my_db, rquery_pipeline_cached, source_limit = 1L))))

# full run
system.time(nrow(as.data.frame(execute(my_db, rquery_pipeline_cached))))

base::date()
```


The introduction of a `dplyr::compute()` node (with the intent of speeding things up through caching)
can be expensive.

```{r dplyrexp_c, error=TRUE}
base::date()

scale <- 0.237

dplyr_pipeline_c <- d_large_tbl %>%
  group_by(subjectID) %>%
  mutate(probability =
           exp(assessmentTotal * scale)/
           sum(exp(assessmentTotal * scale), na.rm = TRUE)) %>%
  arrange(probability, surveyCategory) %>%
  filter(row_number() == n()) %>%
  compute() %>%     # <- inopportune place to try to cache
  ungroup() %>%
  rename(diagnosis = surveyCategory) %>%
  select(subjectID, diagnosis, probability) %>%
  arrange(subjectID)

# print query
cat(dbplyr::remote_query(dplyr_pipeline_c))

# full run
system.time(nrow(as.data.frame(dplyr_pipeline_c)))

base::date()
```



Now, let's show how/where erroneous pipelines are debugged in each system.

In `rquery` many user errors are caught during pipeline construction, 
independent of database.

```{r late_error_rqueryc, error=TRUE}
base::date()

# rquery catches the error during pipeline definition,
# prior to sending it to the database or Spark data system.
rquery_pipeline_late_error <- d_large %.>%
  extend_nse(.,
             probability :=
               exp(assessmentTotal * scale))  %.>% 
  normalize_cols(.,
                 "probability",
                 partitionby = 'subjectID') %.>%
  pick_top_k(.,
             partitionby = 'subjectID',
             rev_orderby = c('probability', 'surveyCategory')) %.>%
  rename_columns(., 'diagnosis' := 'surveyCategory') %.>%
  select_columns(., qc(subjectID, diagnosis, probability)) %.>%
  orderby(., 'ZubjectIDZZZ') # <- error non-existent column

base::date()
```

With `dplyr` user errors are mostly caught when the command is 
analyzed on the remote data system.

```{r late_error_dplyrc, error=TRUE}
base::date()

# dplyr accepts an incorrect pipeline
dplyr_pipeline_late_error <- d_large_tbl %>%
  group_by(subjectID) %>%
  mutate(probability =
           exp(assessmentTotal * scale)/
           sum(exp(assessmentTotal * scale), na.rm = TRUE)) %>%
  arrange(probability, surveyCategory) %>%
  filter(row_number() == n()) %>%
  ungroup() %>%
  rename(diagnosis = surveyCategory) %>%
  select(subjectID, diagnosis, probability) %>%
  arrange(ZubjectIDZZZ)  # <- error non-existent column

# dplyr will generate (incorrect) SQL from the incorrect pipeline
cat(dbplyr::remote_query(dplyr_pipeline_late_error))

# Fortunately, the database's query analyzer does catch the error quickly
# in this case.
system.time(nrow(as.data.frame(dplyr_pipeline_late_error)))

base::date()
```




```{r cleanup}
base::date()
DBI::dbDisconnect(my_db)
base::date()
```

