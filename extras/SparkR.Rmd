---
title: "`rquery`: Practical Big Data Transforms for R Users (with Spark examples)"
author: "Nina Zumel and John Mount, Win-Vector LLC"
output: github_document
date: "07/06/2018"
always_allow_html: yes
---

In this article we will introduce `rquery`, a powerful query tool that allows R users
to implement powerful data transformations using Spark and other big data systems.
`rquery` is based on [Edgar F. Coddâ€™s relational algebra](https://en.wikipedia.org/wiki/Relational_algebra), informed by our experiences using SQL and R packages such as `dplyr` at big data scale.

## Data Transformation and Codd's Relational Algebra

`rquery` is based on an appreciation of Codds' relational algebra.

Codd's relational algebra is a formal algebra that describes the semantics of data transformations and queries. Previous, hierarchical, databases required associations to be represented as functions or maps. Codd relaxed this requirement from functions to relations, allowing tables that represent more powerful associations (allowing, for instance, two-way multimaps). 

Codd's work allows most significant data transformations to be decomposed into sequences made up from a smaller set of fundamental operations: 

  * select (row selection)
  * project (column selection/aggregation)
  * Cartesian product (table joins, row binding, and set difference)
  * extend (derived columns, keyword was in Tutorial-D).

One of the earliest and still most common implementation of Codd's algebra is SQL. Formally Codd's algebra assumes that all rows in a table are unique; SQL further relaxes this restriction to allow multisets.

`rquery` is another realization of the Codd algebra that implements the above operators, some higher-order operators, and emphasizes a right to left pipe notation.  This gives the Spark user an additional way to work effectively.

## SQL vs pipelines for data transformation

Without a pipe-line based operator notation the common ways to control Spark include SQL or sequencing
SparkR data transforms.  `rquery` is a complimentary approach that can be combined with these other methodologies.

One issue with SQL, especially for the novice SQL programmer, is that it can be somewhat unintuitive.

  * SQL expresses data transformations as nested function composition
  * SQL uses some relational concepts as steps, others as modifiers and predicates.

For example, suppose you have a table of information about irises, and you want to find the species with the widest petal on average. In R the steps would be as follows:

  1. Group the table into Species
  2. Calculate the mean petal width for each Species
  3. Find the widest mean petal width
  4. Return the appropriate species
  
We can do this in R using `rqdatatable`, an in-memory implementation of `rquery`:

```{r}
library(rqdatatable)

data(iris)

iris %.>%
  project_nse(., groupby=c('Species'),
              mean_petal_width = mean(Petal.Width)) %.>%
  pick_top_k(.,  
             k = 1,
             orderby = c('mean_petal_width', 'Species'),
             reverse = c('mean_petal_width')) %.>% 
  select_columns(., 'Species') 
```


```{r echo=FALSE, eval=FALSE}
# this won't work if here because rquery not loaded and inited yet

iris_remote <- SparkR::createDataFrame(iris)

# first, we need a name for the table, so we create a view (hide this)
SparkR::createOrReplaceTempView(iris_remote, "iris")
iris_description = rquery::db_td(db_hdl, "iris")

library("rquery")

iris_pipeline <- iris_description %.>%
  project_nse(., groupby=c("Species"),
              mean_petal_width = mean(Petal_Width)) %.>%
  pick_top_k(.,  
             k = 1,
             orderby = c('mean_petal_width', 'Species'),
             reverse = c('mean_petal_width')) %.>% 
  select_columns(., 'Species')

execute(db_hdl, iris_pipeline)
# get rquery sql
cat(format(to_sql(iris_pipeline, db_hdl)))

# get dplyr sql (can't run, becase SQLite is weak)
memdb <- DBI::dbConnect(RSQLite::SQLite(), ":memory:")
memdbtbl <- dplyr::copy_to(memdb, iris)
memdbtbl %>%
  group_by(Species) %>%
  summarize(mean_petal_width = mean(Petal.Width, na.rm=TRUE)) %>%
  mutate(rank = rownumber(-mean_petal_width)) %>%
  filter(rank == 1) %>%
  select(Species) %>%
  dbplyr::remote_query()
```

Of course, we could also do the same operation using `dplyr`, another R package with Codd-style operators. `rquery` has some advantages we will discuss later.

In `rquery`, the original table (`iris`) is at the beginning of the query, with successive operations applied to the results of the preceding line.  To perform the equivalent operation in SQL, you must write down the operation essentially backwards:

```
SELECT 
   Species 
FROM (
   SELECT 
     Species, 
     mean('Petal.Width') AS mean_petal_width 
   FROM 
     iris
   GROUP BY Species ) tmp1
WHERE mean_petal_width = max(mean_petal_width) /* try to get widest species */
ORDER_BY Species /* To make tiebreaking deterministic */
LIMIT 1     /* Get only one species back (in case of ties) */
```

In SQL the original table is in the *last* or inner-most SELECT statement, with successive results nested up from there. In addition, column selection directives are at the beginning of a SELECT statement, while row selection criteria (WHERE, LIMIT) and modifiers (GROUP_BY, ORDER_BY) are at the end of the statement, with the table in between. So the data transformation goes from the inside of the query to the outside, which can be hard to read -- not to mention hard to write.

`rquery` represents an attempt to make data transformation in a relational database more intuitive by expressing data transformations as a sequential operator pipeline instead of nested queries or functions. 

## `rquery` for Spark/R developers

For developers working with Spark and R, `rquery` offers a number of advantages. First, R developers can run analyses and perform data transformations in Spark using an easier to read (and to write) sequential pipeline notation instead of nested SQL queries. As we mentioned above, `dplyr` also supplies this capability, but `dplyr` is not compatible with `SparkR` -- only with `sparklyr`. `rquery` is compatible with both `SparkR` and `sparklyr`, as well as with Postgres and other large data stores. In addition, `dplyr`'s lazy evaluation can complicate the running and debugging of large, complex queries (more on this below).

The design of `rquery` is *database-first*, meaning it was developed specifically to address issues that arise when working with big data in remote data stores via R. `rquery` maintains *complete separation between the query specification and query execution phases*, which allows useful error-checking and some optimization before the query is run. This can be valuable when running complex queries on large volumes of data; you don't want to run a long query only to discover that there was an obvious error on the last step.

`rquery` checks column names at query specification time to insure that they are available for use. It also keeps track of which columns from a table are involved with a given query, and proactively issues the appropriate SELECT statements to narrow the tables being manipulated (this is not very important on Spark due to its columnar orientation and lazy evaluation semantics, but can be a key on other data store). This can help speed up queries that involve excessively wide tables where only a few columns are needed.

`rquery` also offers well-formatted textual as well as graphical presentation of query plans. In addition, you can inspect the generated SQL query before execution.


## Example 

```{r echo=FALSE, eval=FALSE}
# Connect to a `SparkR` cluster and work a small example.
# 
# To install a practice version of `Spark`/`SparkR` v2.3.0 on a stand-alone workstation:
# 
#   * First download Spark 2.3.0 Pre-built for Apache Hadoop 2.7 or later ([spark-2.3.0-bin-hadoop2.7.tgz](https://www.apache.org/dyn/closer.lua/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz)) from [Apache Spark Downloads](https://spark.apache.org/downloads.html).
#   * Uncompress this into a directory named `spark-2.3.0-bin-hadoop2.7`.
#   * Install `SparkR` from `spark-2.3.0-bin-hadoop2.7/spark-2.3.0-bin-hadoop2.7/R/lib/SparkR`: `install.packages("~/Downloads/spark-2.3.0-bin-hadoop2.7/R/lib/SparkR/", repos = NULL, type = "source")`.
#   * Use `SparkR` package to install its own local `Spark`: `SparkR::install.spark()` (based on [sparkr-vignettes.Rmd](https://github.com/apache/spark/blob/master/R/pkg/vignettes/sparkr-vignettes.Rmd)).

```


```{r start_sparkr, include=FALSE}
# set up connection to Spark via SparkR

# From SparkR package vignette/README
# https://github.com/apache/spark/blob/master/R/pkg/vignettes/sparkr-vignettes.Rmd
knitr::opts_hooks$set(eval = function(options) {
  # override eval to FALSE only on windows
  if (.Platform$OS.type == "windows") {
    options$eval = FALSE
  }
  options
})
r_tmp_dir <- tempdir()
tmp_arg <- paste0("-Djava.io.tmpdir=", r_tmp_dir)
sparkSessionConfig <- list(spark.driver.extraJavaOptions = tmp_arg,
                           spark.executor.extraJavaOptions = tmp_arg)
old_java_opt <- Sys.getenv("_JAVA_OPTIONS")
Sys.setenv("_JAVA_OPTIONS" = paste("-XX:-UsePerfData", old_java_opt, sep = " "))
ses <- SparkR::sparkR.session(master = "local[1]", 
                              sparkConfig = sparkSessionConfig, 
                              enableHiveSupport = FALSE)
```

For our next example let's imagine that we run a food delivery business, and we are interested in what types of cuisines ('Mexican', 'Chinese', etc) our customers prefer. We want to sum up the number of orders of each cuisine type (or `restaurant_type`) by customer and compute which cuisine appears to be their favorite, based on what they order the most. We also want to see how strong that preference is, based on what fraction of their orders is of their favorite cuisine.

We'll start with a table of orders, which records order id, customer id, and restaurant type.

```{r build_data, include=FALSE}
# insert example data, simulating it already being on the remote system.

# From: https://github.com/WinVector/rquery/blob/master/extras/DebugToolsForBigData.md
set.seed(235235)
nSubj <- 9

d_local <- data.frame(custID = paste0("cust_", sample.int(nSubj, 244, replace=TRUE)),
                      stringsAsFactors = FALSE)
                        
d_local$restaurant_type = sample( c(
                   'Mexican',
                   'Italian',
                   'Chinese',
                   'Indian',
                   'American'
                   ), nrow(d_local), replace=TRUE)
d_local$orderID = seq_len(nrow(d_local))
```

```{r, echo=FALSE}
knitr::kable(head(d_local))
```

```{r connect_rquery, include=FALSE}
#
# push the data into Spark and get a name for it
#

test_df <- SparkR::createDataFrame(d_local)
# https://github.com/apache/spark/blob/master/examples/src/main/r/RSparkSQLExample.R
# SparkR::createOrReplaceTempView(test_df, "table")
# SparkR::collect(SparkR::sql("SELECT * from table"))

# first, we need a name for the table, so we create a view )
SparkR::createOrReplaceTempView(test_df, "order_table")

#
# set up SparkR adapter functions, to make interaction simple.
#

# define SparkR cluster adapting handle
# this overrides all rquery functions need help.
# this would eventually be in an adapter package.
# SparkR is not DBI- so we supply specific (non-DBI) solutions.
db_hdl <- rquery::rquery_db_info(
  connection = ses,
  is_dbi = FALSE,
  indentifier_quote_char = '`',
  string_quote_char = '"',
  note = "SparkR",
  overrides = list(
    rq_get_query = function(db, q) {
      SparkR::collect(SparkR::sql(q))
    },
    rq_execute = function(db, q) {
      SparkR::sql(q)
    },
    rq_colnames = function(db, table_name) {
      q <- paste0("SELECT * FROM ",
                  rquery::quote_identifier(db, table_name),
                  " LIMIT 1")
      v <- rquery::rq_get_query(db, q)
      colnames(v)
    }
  ))
db_hdl$quote_identifier <- function(x, id) {
  db_hdl$dbqi(id)
}
db_hdl$quote_string <- function(x, s) {
  db_hdl$dbqs(s)
}
db_hdl$quote_literal <- function(x, o) {
  if(is.character(o) || is.factor(o)) {
    return(db_hdl$dbqs(as.character(o)))
  }
  db_hdl$dbql(o)
}
```

To work with the data using `rquery`, we need an `rquery` handle to the Spark cluster. Since `rquery` interfaces with many different types of SQL-dialect data stores, it needs an adapter to translate `rquery` functions into the appropriate SQL dialect. The default handler assumes a DBI-adapted database. Since `SparkR` is not DBI-adapted, we must define the handler explicitly, using the function `rquery::rquery_db_info()`. The code for the adapter is here [LINK]. Let's assume that we have created the handler as `db_hdl`.

```{r example}
library("rquery")

print(db_hdl) # rquery handle into Spark

```

Let's assume that we already have the data in Spark, as `order_table`. To work with the table in `rquery`, we must generate a 
*table description*, using the function `db_td()`. A table description is a record of the table's name and columns; `db_td()` queries the database to get the description.

```{r}
table_description = db_td(db_hdl, "order_table")

print(table_description)
print(column_names(table_description))
```

Now we can compose the necessary processing pipeline (or *operator tree*), using `rquery`'s Codd-style steps and the pipe notation:

```{r}
rquery_pipeline <- table_description %.>%
  extend_nse(., one = 1) %.>%  # a column to help count
  project_nse(., groupby=c("custID", "restaurant_type"),
              total_orders = sum(one)) %.>% # sum the orders of each type, per customer
  normalize_cols(.,   # normalize the total_order counts
                 "total_orders",
                 partitionby = 'custID') %.>%
  rename_columns(.,  # rename the column
                 c('fraction_of_orders' = 'total_orders')) %.>% 
  pick_top_k(.,  # get the most frequent cuisine type
             k = 1,
             partitionby = 'custID',
             orderby = c('fraction_of_orders', 'restaurant_type'),
             reverse = c('fraction_of_orders')) %.>% 
  rename_columns(., c('favorite_cuisine' = 'restaurant_type')) %.>%
  select_columns(., c('custID', 
                      'favorite_cuisine', 
                      'fraction_of_orders')) %.>%
  orderby(., cols = 'custID')
```

Before executing the pipeline, you can inspect it, either as text:

```{r}
cat(format(rquery_pipeline))
```

or as a operator diagram (using the package `DiagrammeR`). This is especially useful for complex queries that involve multiple tables.

```{r echo=TRUE, eval=FALSE }
rquery_pipeline %.>%
  op_diagram(.) %.>% 
  DiagrammeR::DiagrammeR(diagram = ., type = "grViz")
```

```{r echo=FALSE, eval=TRUE}
rquery_pipeline %.>%
  op_diagram(.) %.>% 
  DiagrammeR::DiagrammeR(diagram = ., type = "grViz") %.>% 
  DiagrammeRsvg::export_svg(.) %.>% 
  charToRaw(.) %.>%
  rsvg::rsvg_png(., file = "Sparkr_files/diagram1.png")
```

![](Sparkr_files/diagram1.png)

Notice that the `normalize_cols` and `pick_top_k` steps were decomposed into more basic Codd operators (for example, the *extend* and *select_rows* nodes). 

You can also inspect what tables are used in the pipeline, and which columns in those tables are involved.

```{r}
tables_used(rquery_pipeline)

columns_used(rquery_pipeline)
```

If you want, you can inspect the (complex and heavily nested) SQL query that will be executed in the cluster. Notice that the column `orderID`, which is not involved in this query, is already eliminated in the initial SELECT (tsql_*_0000000000). Winnowing the initial tables down to only the columns used can be a big performance improvement when you are working with excessively wide tables, and using only a few columns.

```{r}
cat(to_sql(rquery_pipeline, db_hdl))
```

Finally, we can execute the query in the cluster. Note that this same pipeline could also be executed using a `sparklyr` connection.

```{r}
execute(db_hdl, rquery_pipeline) %.>%
  knitr::kable(.)
```

`execute()` brings results from Spark to R, and the related command `materialize()` lands
results in Spark without transporting data to R.

Since many R programmers are familiar with `dplyr`, we can compare `dplyr`'s operators to those of `rquery` (and verify that the calculation is correct), by processing a local copy of the data with `dplyr`. 

```{r}
library(dplyr)

# get the SparkDataFrame by name
order_f = SparkR::tableToDF("order_table")

order_f %>% 
  SparkR::as.data.frame() %>%
  group_by(custID, restaurant_type) %>%
  summarize(total_orders = n()) %>%
  ungroup() %>%
  group_by(custID) %>%
  mutate(fraction_of_orders = total_orders/sum(total_orders)) %>%
  ungroup() %>%
  group_by(custID) %>%
  arrange(desc(fraction_of_orders), restaurant_type) %>%
  mutate(rnk = row_number()) %>%
  ungroup() %>%
  filter(rnk == 1) %>%
  rename(favorite_cuisine = restaurant_type) %>%
  select(custID, favorite_cuisine, fraction_of_orders) %>%
  arrange(custID) %>%
  knitr::kable()
```

Notice small sequences of `dplyr` operators are used to implement basic transforms.

As previously mentioned, `rquery` can work with a variety of SQL-backed data stores; you only need an appropriate adapter. In an earlier example, we showed `rqdatatable`, an adaptation of the `rquery` grammar for in-memory use, using `datatable` as the back-end implementation. 

One of the cool features of the `rquery` grammar is that the `rquery` operator trees and pipelines are back-end independent. This means you can use the pipeline that we created above with Spark through either `SparkR` or `sparklyr`, or on another data source like Postgres (assuming the table on Postgres has the same structure). You can also use the pipeline on local copies of the data with `rqdatatable`, as we show below.

Note that you MUST use "%.>%" (aka the "dot-arrow" from the `wrapr` package) rather than the `magrittr` pipe for this next step (though there are also convenient non-pipe methods for producing the same result):

```{r}
library(rqdatatable)  
# this automatically registers rqdatatable as the default executor 
# for rquery pipelines

# simulate taking a rehearsal subset of data
local_copy <- SparkR::as.data.frame(SparkR::head(order_f, n = 10000))
class(local_copy)

local_copy %.>%
  rquery_pipeline %.>%
  knitr::kable(.)

```

This feature can be quite useful for "rehearsals" of complex data manipulation/analysis processes, where you'd like to develop and debug the data process quickly, using smaller local versions of the relevant data tables before executing them on the remote system.

```{r cleanup, include=FALSE}
SparkR::sparkR.session.stop()
```

## Conclusion

`rquery` is a powerful "database first" piped query generator.  It includes a number of 
useful documentation, debugging, and optimization features.  It makes working with big data
much easier and works with many systems including SparkR, sparklyr, and PostgreSQL; meaning
`rquery` does not usurp your design decisions or choice of platform.


