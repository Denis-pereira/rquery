---
title: "SparkR Example"
author: "Nina Zumel and John Mount, Win-Vector LLC"
output: github_document
date: "07/03/2018"
always_allow_html: yes
---

## The What and Why of `rquery`

`rquery` is a query generator for R. It is based on [Edgar F. Coddâ€™s relational algebra](https://en.wikipedia.org/wiki/Relational_algebra), informed by
our experience using SQL and R packages such as `dplyr` at big data scale. 

The design represents an attempt to make SQL more teachable by denoting composition by a sequential pipeline notation 
instead of nested queries or functions. 

A great benefit of Codd's relational algebra is it gives one concepts to decompose complex data transformations into sequences of simpler transformations.

Some reasons SQL seems complicated include:

    SQL's realization of sequencing as nested function composition.
    SQL uses some relational concepts as steps, others as modifiers and predicates.

A lot of the grace of the Codd theory can be recovered through the usual trick changing function composition notation from g(f(x)) to x . f() . g(). This experiment is asking (and not for the first time): "what if SQL were piped (expressed composition as a left to right flow, instead of a right to left nesting)?"


## Design Choices for SQL implementation

Stay close to Cod's definitions. In contrast to other R data manipulation packages, rquery does not rely on db-unsafe annotations such as row number or "group by" annotations. Instead, grouped aggregations are implemented via window functions in SQL. 


## How Spark & R developers can benefit from `rquery`

Domain independent query language that runs on Spark in R via either SparkR or sparklyr, as well as on Postgres and other large data systems.

Superior error checking -- verifies column names before going to database

Some query optimization; in particular rquery checks "column liveness" : it checks which columns from a table are involved in a given query, and proactively issues the approprate SELECT statements to narrow the tables being manipulated. Helps with excessively wide tables.

Well-formatted textual as well as graphicl presentation of query plans.

## Example 


Connect to a `SparkR` cluster and work a small example.

To install a practice version of `Spark`/`SparkR` v2.3.0 on a stand-alone workstation:

  * First download Spark 2.3.0 Pre-built for Apache Hadoop 2.7 or later ([spark-2.3.0-bin-hadoop2.7.tgz](https://www.apache.org/dyn/closer.lua/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz)) from [Apache Spark Downloads](https://spark.apache.org/downloads.html).
  * Uncompress this into a directory named `spark-2.3.0-bin-hadoop2.7`.
  * Install `SparkR` from `spark-2.3.0-bin-hadoop2.7/spark-2.3.0-bin-hadoop2.7/R/lib/SparkR`: `install.packages("~/Downloads/spark-2.3.0-bin-hadoop2.7/R/lib/SparkR/", repos = NULL, type = "source")`.
  * Use `SparkR` package to install its own local `Spark`: `SparkR::install.spark()` (based on [sparkr-vignettes.Rmd](https://github.com/apache/spark/blob/master/R/pkg/vignettes/sparkr-vignettes.Rmd)).

TODO: switch to number of times ordering from different restaurants.  Looking for the favorite and fraction of orders from orders from that restaunt (or quisine).  Slow down and define problem and show data, before any work.



```{r start_sparkr, include=FALSE}
# set up connection to Spark via SparkR

# From SparkR package vignette/README
# https://github.com/apache/spark/blob/master/R/pkg/vignettes/sparkr-vignettes.Rmd
knitr::opts_hooks$set(eval = function(options) {
  # override eval to FALSE only on windows
  if (.Platform$OS.type == "windows") {
    options$eval = FALSE
  }
  options
})
r_tmp_dir <- tempdir()
tmp_arg <- paste0("-Djava.io.tmpdir=", r_tmp_dir)
sparkSessionConfig <- list(spark.driver.extraJavaOptions = tmp_arg,
                           spark.executor.extraJavaOptions = tmp_arg)
old_java_opt <- Sys.getenv("_JAVA_OPTIONS")
Sys.setenv("_JAVA_OPTIONS" = paste("-XX:-UsePerfData", old_java_opt, sep = " "))
ses <- SparkR::sparkR.session(master = "local[1]", 
                              sparkConfig = sparkSessionConfig, 
                              enableHiveSupport = FALSE)
```

Introduce the data here

```{r build_data, include=FALSE}
# insert example data, simulating it already being on the remote system.

# From: https://github.com/WinVector/rquery/blob/master/extras/DebugToolsForBigData.md
set.seed(235235)
nSubj <- 10
d_local <- data.frame(custID = sort(rep(seq_len(nSubj),5)),
                 restaurantType = c(
                   'Mexican',
                   'Italian',
                   'Chinese',
                   'Indian',
                   'American'
                   ),
                 stringsAsFactors = FALSE)
d_local$totalOrders <- sample.int(25, nrow(d_local), replace = TRUE)

test_df <- SparkR::createDataFrame(d_local)
# https://github.com/apache/spark/blob/master/examples/src/main/r/RSparkSQLExample.R
# SparkR::createOrReplaceTempView(test_df, "table")
# SparkR::collect(SparkR::sql("SELECT * from table"))

# first, we need a name for the table, so we create a view (hide this)
SparkR::createOrReplaceTempView(test_df, "survey_table")

knitr::kable(head(d_local))
```

[`rquery`](https://winvector.github.io/rquery/) example.

```{r connect_rquery, include=FALSE}
# set up SparkR adapter functions, to make interaction simple.

# define SparkR cluster adapting handle
# this overrides all rquery functions need help.
# this would eventually be in an adapter package.
# SparkR is not DBI- so we supply specific (non-DBI) solutions.
db_hdl <- rquery::rquery_db_info(
  connection = ses,
  is_dbi = FALSE,
  indentifier_quote_char = '`',
  string_quote_char = '"',
  note = "SparkR",
  overrides = list(
    rq_get_query = function(db, q) {
      SparkR::collect(SparkR::sql(q))
    },
    rq_execute = function(db, q) {
      SparkR::sql(q)
    },
    rq_colnames = function(db, table_name) {
      q <- paste0("SELECT * FROM ",
                  rquery::quote_identifier(db, table_name),
                  " LIMIT 1")
      v <- rquery::rq_get_query(db, q)
      colnames(v)
    }
  ))
db_hdl$quote_identifier <- function(x, id) {
  db_hdl$dbqi(id)
}
db_hdl$quote_string <- function(x, s) {
  db_hdl$dbqs(s)
}
db_hdl$quote_literal <- function(x, o) {
  if(is.character(o) || is.factor(o)) {
    return(db_hdl$dbqs(as.character(o)))
  }
  db_hdl$dbql(o)
}
```

Let's assume that we already have the data

```{r example}
library("rquery")

print(db_hdl) # rquery handle into Spark


survey_f <- SparkR::tableToDF("survey_table")
print(survey_f)

#
# create an rquery table description of the SparkDataFrame
#

# get the table's column names
cols <- SparkR::colnames(survey_f)
# create the rquery table description
table_description <- mk_td("survey_table", cols)

print(table_description)
print(column_names(table_description))

rquery_pipeline <- table_description %.>%
  normalize_cols(.,   # normalize the restaurantType counts
                 "totalOrders",
                 partitionby = 'custID') %.>%
  rename_columns(.,  # rename the column
                 c('fraction_of_orders' = 'totalOrders')) %.>% 
  pick_top_k(.,
             k = 1,
             partitionby = 'custID',
             orderby = c('fraction_of_orders', 'restaurantType'),
             reverse = c('fraction_of_orders')) %.>% 
  rename_columns(., c('favorite_cuisine' = 'restaurantType')) %.>%
  select_columns(., c('custID', 
                      'favorite_cuisine', 
                      'fraction_of_orders')) %.>%
  orderby(., cols = 'custID')
```


```{r}
cat(format(rquery_pipeline))

rquery_pipeline %.>%
  op_diagram(.) %.>% 
  DiagrammeR::DiagrammeR(diagram = ., type = "grViz") %.>% 
  DiagrammeRsvg::export_svg(.) %.>% 
  charToRaw(.) %.>%
  rsvg::rsvg_png(., file = "Sparkr_files/diagram1.png")
```

![](Sparkr_files/diagram1.png)

```{r}
columns_used(rquery_pipeline)
```


```{r}
execute(db_hdl, rquery_pipeline) %.>%
  knitr::kable(.)
```


```{r cleanup, include=FALSE}
SparkR::sparkR.session.stop()
```
