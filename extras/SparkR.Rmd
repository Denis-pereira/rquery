---
title: "SparkR Example"
author: "John Mount, Win-Vector LLC"
output: github_document
date: "06/02/2018"
always_allow_html: yes
---

Connect to a `SparkR` cluster and work a small example.

To install a practice version of `Spark`/`SparkR` v2.3.0 on a stand-alone workstation:

  * First download Spark 2.3.0 Pre-built for Apache Hadoop 2.7 or later ([spark-2.3.0-bin-hadoop2.7.tgz](https://www.apache.org/dyn/closer.lua/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz)) from [Apache Spark Downloads](https://spark.apache.org/downloads.html).
  * Uncompress this into a directory named `spark-2.3.0-bin-hadoop2.7`.
  * Install `SparkR` from `spark-2.3.0-bin-hadoop2.7/spark-2.3.0-bin-hadoop2.7/R/lib/SparkR`: `install.packages("~/Downloads/spark-2.3.0-bin-hadoop2.7/R/lib/SparkR/", repos = NULL, type = "source")`.
  * Use `SparkR` package to install its own local `Spark`: `SparkR::install.spark()` (based on [sparkr-vignettes.Rmd](https://github.com/apache/spark/blob/master/R/pkg/vignettes/sparkr-vignettes.Rmd)).

TODO: switch to number of times ordering from different restaurants.  Looking for the favorite and fraction of orders from orders from that restaunt (or quisine).  Slow down and define problem and show data, before any work.



```{r start_sparkr, include=FALSE}
# set up connection to Spark via SparkR

# From SparkR package vignette/README
# https://github.com/apache/spark/blob/master/R/pkg/vignettes/sparkr-vignettes.Rmd
knitr::opts_hooks$set(eval = function(options) {
  # override eval to FALSE only on windows
  if (.Platform$OS.type == "windows") {
    options$eval = FALSE
  }
  options
})
r_tmp_dir <- tempdir()
tmp_arg <- paste0("-Djava.io.tmpdir=", r_tmp_dir)
sparkSessionConfig <- list(spark.driver.extraJavaOptions = tmp_arg,
                           spark.executor.extraJavaOptions = tmp_arg)
old_java_opt <- Sys.getenv("_JAVA_OPTIONS")
Sys.setenv("_JAVA_OPTIONS" = paste("-XX:-UsePerfData", old_java_opt, sep = " "))
ses <- SparkR::sparkR.session(master = "local[1]", 
                              sparkConfig = sparkSessionConfig, 
                              enableHiveSupport = FALSE)
```


```{r build_data, include=FALSE}
# insert example data, simulating it already being on the remote system.

# From: https://github.com/WinVector/rquery/blob/master/extras/DebugToolsForBigData.md
set.seed(235235)
nSubj <- 10
d_local <- data.frame(subjectID = sort(rep(seq_len(nSubj),2)),
                 surveyCategory = c(
                   'withdrawal behavior',
                   'positive re-framing'),
                 stringsAsFactors = FALSE)
d_local$assessmentTotal <- sample.int(10, nrow(d_local), replace = TRUE)
irrel_col_1 <- paste("irrelevantCol", sprintf("%07g", 1), sep = "_")
d_local[[irrel_col_1]] <- runif(nrow(d_local))
test_df <- SparkR::createDataFrame(d_local)
# https://github.com/apache/spark/blob/master/examples/src/main/r/RSparkSQLExample.R
# SparkR::createOrReplaceTempView(test_df, "table")
# SparkR::collect(SparkR::sql("SELECT * from table"))
```

[`rquery`](https://winvector.github.io/rquery/) example.

```{r connect_rquery, include=FALSE}
# set up SparkR adapter functions, to make interaction simple.

#' Take a SparkR remote handle and build an rquery table description, with a new 
#' chosen table name.
#'
#' The issue is: we don't know how to get or set the name of SparkDataFrame, so
#' we create a view representation of it, where we can set (and therefore know)
#' the name of the table.
#' 
#' @param df SparkDataFrame remote data handle.
#' @param nam character, name to use for new rquery remote object handle.
#' @return rquery table description.
#'
sparkr_table <- function(df, 
                         nam) {
  SparkR::createOrReplaceTempView(df, nam)
  cols <- SparkR::colnames(df)
  mk_td(nam, cols)
}

# define SparkR cluster adapting handle
# this overrides all rquery functions need help.
# this would eventually be in an adapter package.
# SparkR is not DBI- so we supply specific (non-DBI) solutions.
db_hdl <- rquery::rquery_db_info(
  connection = ses,
  is_dbi = FALSE,
  indentifier_quote_char = '`',
  string_quote_char = '"',
  note = "SparkR",
  overrides = list(
    rq_get_query = function(db, q) {
      SparkR::collect(SparkR::sql(q))
    },
    rq_execute = function(db, q) {
      SparkR::sql(q)
    },
    rq_colnames = function(db, table_name) {
      q <- paste0("SELECT * FROM ",
                  rquery::quote_identifier(db, table_name),
                  " LIMIT 1")
      v <- rquery::rq_get_query(db, q)
      colnames(v)
    }
  ))
db_hdl$quote_identifier <- function(x, id) {
  db_hdl$dbqi(id)
}
db_hdl$quote_string <- function(x, s) {
  db_hdl$dbqs(s)
}
db_hdl$quote_literal <- function(x, o) {
  if(is.character(o) || is.factor(o)) {
    return(db_hdl$dbqs(as.character(o)))
  }
  db_hdl$dbql(o)
}
```

```{r example}
library("rquery")

print(db_hdl)
print(test_df)

table_description <- sparkr_table(test_df, "survey_table")

print(table_description)
print(column_names(table_description))

rquery_pipeline <- table_description %.>%
  extend_nse(.,
             assessment_fraction = assessmentTotal)  %.>% 
  normalize_cols(.,
                 "assessment_fraction",
                 partitionby = 'subjectID') %.>%
  pick_top_k(.,
             k = 1,
             partitionby = 'subjectID',
             orderby = c('assessment_fraction', 'surveyCategory'),
             reverse = c('assessment_fraction', 'surveyCategory')) %.>% 
  rename_columns(., c('favorite_category' = 'surveyCategory')) %.>%
  select_columns(., c('subjectID', 
                      'favorite_category', 
                      'assessment_fraction')) %.>%
  orderby(., cols = 'subjectID')
```


```{r}
cat(format(rquery_pipeline))

rquery_pipeline %.>%
  op_diagram(.) %.>% 
  DiagrammeR::DiagrammeR(diagram = ., type = "grViz") %.>% 
  DiagrammeRsvg::export_svg(.) %.>% 
  charToRaw(.) %.>%
  rsvg::rsvg_png(., file = "Sparkr_files/diagram1.png")
```

![](Sparkr_files/diagram1.png)

```{r}
columns_used(rquery_pipeline)
```


```{r}
execute(db_hdl, rquery_pipeline) %.>%
  knitr::kable(.)
```


```{r cleanup, include=FALSE}
SparkR::sparkR.session.stop()
```
