---
title: "data.table backend for rquery"
author: "John Mount, Win-Vector LLC"
date: "05/30/2018"
output: github_document
---

We can work an example similar to the [`rquery`](https://winvector.github.io/rquery/) [example](https://winvector.github.io/rquery/index.html) using a [`data.table`](http://r-datatable.com/) 
back-end ([`qdatatable`](https://github.com/WinVector/qdatatable)).

```{r packages}
library("ggplot2")
library("microbenchmark")
library("dplyr")
library("dtplyr")
# https://github.com/WinVector/qdatatable
library("qdatatable") # devtools::install.packages("WinVector/qdatatable")
packageVersion("dplyr")
packageVersion("data.table")
packageVersion("rquery")
```


```{r example1}
# data example
set.seed(2362)
mk_example <- function(nsubjects, nirrelcols) {
  d <- rbind(data.frame(subjectID = seq_len(nsubjects), 
                        surveyCategory = "withdrawal behavior",
                        stringsAsFactors = FALSE),
             data.frame(subjectID = seq_len(nsubjects), 
                        surveyCategory = "positive re-framing",
                        stringsAsFactors = FALSE))
  d <- d[order(d$subjectID, d$surveyCategory), , drop = FALSE]
  d$assessmentTotal <- rbinom(nrow(d), 10, 0.3)
  for(i in seq_len(nirrelcols)) {
    d[[paste0("irrelevantCol_", i)]] <- runif(nrow(dL))
  }
  rownames(d) <- NULL
  d
}

dL <- mk_example(2, 0)
```


```{r rqueryp}
scale <- 0.237

# example rquery pipeline
rquery_pipeline <- local_td(dL) %.>%
  extend_nse(.,
             probability :=
               exp(assessmentTotal * scale))  %.>% 
  normalize_cols(.,
                 "probability",
                 partitionby = 'subjectID') %.>%
  pick_top_k(.,
             partitionby = 'subjectID',
             orderby = c('probability', 'surveyCategory'),
             reverse = c('probability', 'surveyCategory')) %.>% 
  rename_columns(., 'diagnosis' := 'surveyCategory') %.>%
  select_columns(., c('subjectID', 
                      'diagnosis', 
                      'probability')) %.>%
  orderby(., cols = 'subjectID')
```

Show expanded form of query tree.

```{r printrqueryp, comment=""}
cat(format(rquery_pipeline))
```

Execute `rquery` pipeline using `data.table` as the implementation.

```{r runrqueryp}
ex_data_table(rquery_pipeline) %.>%
  knitr::kable(.)
```

Execute `rquery` pipeline using `PostgreSQL` as the implementation.

```{r rquerydb}
# configure a database connection
my_db <- DBI::dbConnect(RPostgreSQL::PostgreSQL(),
                          host = 'localhost',
                          port = 5432,
                          user = 'johnmount',
                          password = '')
dbopts <- rq_connection_tests(my_db)
options(dbopts)
# build the shared handle
winvector_temp_db_handle <- list(db = my_db)

# run the job
execute(dL, rquery_pipeline) %.>%
  knitr::kable(.)
```


`dplyr` pipeline.


```{r dplyrp}
scale <- 0.237

dplyr_pipeline <- . %>% 
  as.tbl(.[, c("subjectID", "surveyCategory", "assessmentTotal")]) %>% # narrow and convert to preferred structure
  group_by(subjectID) %>%
  mutate(probability =
           exp(assessmentTotal * scale)/
           sum(exp(assessmentTotal * scale), na.rm = TRUE)) %>%
  arrange(probability, surveyCategory) %>%
  filter(row_number() == n()) %>%
  ungroup() %>%
  rename(diagnosis = surveyCategory) %>%
  select(subjectID, diagnosis, probability) %>%
  arrange(subjectID) 

dL %>% 
  dplyr_pipeline %>%
  knitr::kable()
```

Try `dtplyr`.

```{r dtplyr, error = TRUE}
data.table::as.data.table(dL) %>% 
  dplyr_pipeline
```

Idiomatic `data.table` pipeline.

```{r datatablef}
# improved code from:
# http://www.win-vector.com/blog/2018/01/base-r-can-be-fast/#comment-66746
data.table_local <- function(dL) {
  # data.table is paying for this copy in its timings (not quite fair)
  # so we will try to minimize it by narrowing columns.
  dDT <- data.table::as.data.table(dL[, c("subjectID", "surveyCategory", "assessmentTotal")])
  dDT <- dDT[, list(diagnosis = surveyCategory,
                    probability = exp (assessmentTotal * scale ) /
                      sum ( exp ( assessmentTotal * scale ) ))
             , subjectID ]
  data.table::setorder(dDT, subjectID, probability, -diagnosis)
  dDT <- dDT[, .SD[.N], subjectID]
  data.table::setorder(dDT, subjectID)
}

data.table_local(dL) %.>%
  knitr::kable(.)
```


Timings on a larger example.

```{r mklargeex}
dL <- mk_example(1000000, 10)
```


```{r confirm}
# show we are working on the new larger data and results agree
dLorig <- dL

ref <- as.data.frame(ex_data_table(rquery_pipeline))
assertthat::assert_that(min(ref$probability)>=0.5) # sensible effect

c1 <- as.data.frame(execute(dL, rquery_pipeline))
assertthat::are_equal(ref, c1)

c2 <- as.data.frame(dplyr_pipeline(dL))
assertthat::are_equal(ref, c2)

c3 <- as.data.frame(data.table_local(dL))
assertthat::are_equal(ref, c3)

# confirm no side-effects back to orginal frame
assertthat::are_equal(dLorig, dL)
```

```{r time}
timings <- microbenchmark(times = 10L,
  rquery_database = nrow(execute(dL, rquery_pipeline)),
  rquery_data.table = nrow(ex_data_table(rquery_pipeline)),
  data.table = nrow(data.table_local(dL)),
  dplyr = nrow(dplyr_pipeline(dL)))
```


```{r presenttimings}
print(timings)

# summarize by hand using rquery database connector
summary_pipeline <- timings %.>%
  as.data.frame(.) %.>%
  project_nse(., groupby = "expr", mean = avg(time)) 
timings %.>% 
  as.data.frame(.) %.>%
  summary_pipeline %.>%
  knitr::kable(.)

autoplot(timings)

WVPlots::ScatterBoxPlotH(as.data.frame(timings), 
                         "time", "expr", 
                         "runtime by implementation in nanoseconds")
```

For more timings (including fast base-R implementations), please see [here](https://github.com/WinVector/rquery/blob/master/extras/QTimingFollowup/QTiming4.md).

```{r cleanup, include=FALSE}
rm(list = "winvector_temp_db_handle")
DBI::dbDisconnect(my_db)
```



